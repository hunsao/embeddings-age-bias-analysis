{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c4ae9e",
   "metadata": {},
   "source": [
    "# TSNE - IMAGE/TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el modelo CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# 2. Configuración de tus imágenes y etiquetas\n",
    "image_dir = \"generated_images/500p_quadruplets_v1\" \n",
    "num_images = 0\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        num_images += 1\n",
    "\n",
    "labels = []  # Lista para las etiquetas de imágenes\n",
    "descriptions = []  # Lista para las descripciones (textos)\n",
    "groups = [] # Lista para los grupos de imagenes\n",
    "\n",
    "image_features_list = [] # Lista para guardar solo los image features\n",
    "text_features_list = [] # Lista para guardar solo los text features\n",
    "\n",
    "# 3. Procesamiento de imágenes y etiquetas\n",
    "for i, filename in enumerate(os.listdir(image_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Carga la imagen\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except Image.UnidentifiedImageError: \n",
    "            print(f\"No se puede abrir o identificar la imagen: {filename}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocesa la imagen con CLIP\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Extrae el embedding de la imagen\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding de la imagen\n",
    "        image_features = image_features / np.linalg.norm(image_features)\n",
    "        image_features_list.append(image_features.flatten()) # Añade a la lista de image features\n",
    "\n",
    "        # Obtener el label del archivo. \n",
    "        label = filename.split(\".\")[0] # Usamos el nombre del archivo sin extension como label\n",
    "        labels.append(label)\n",
    "\n",
    "        # Crea la descripción basada en tu label (Ajusta esto según tus necesidades)\n",
    "        description = f\"{label}\"\n",
    "        descriptions.append(description)\n",
    "\n",
    "        # Prepara la entrada de texto con CLIP\n",
    "        text_input = clip.tokenize([description]).to(device)\n",
    "\n",
    "        # Extrae el embedding del texto\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding del texto\n",
    "        text_features = text_features / np.linalg.norm(text_features)\n",
    "        text_features_list.append(text_features.flatten()) # Añade a la lista de text features\n",
    "\n",
    "        # Clasifica la imagen en un grupo (Ajusta esto según tu estructura de nombres de archivo)\n",
    "        if \"older\" in label.lower():\n",
    "            group = \"older\"\n",
    "        elif \"middle\" in label.lower():\n",
    "            group = \"middle-aged\"\n",
    "        elif \"young\" in label.lower():\n",
    "            group = \"young\"\n",
    "        elif \"person\" in label.lower():\n",
    "            group = \"person\"\n",
    "        else:\n",
    "            group = \"unknown\" # Default group if none of the keywords are found\n",
    "        groups.append(group)\n",
    "\n",
    "print(\"Verificación de la clasificación de grupos:\")\n",
    "for i in range(len(labels)):\n",
    "    print(f\"Label: {labels[i]}, Group: {groups[i]}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "image_features_array = np.array(image_features_list)\n",
    "text_features_array = np.array(text_features_list)\n",
    "\n",
    "# 4. Reducción de dimensionalidad con t-SNE para IMAGENES\n",
    "reduction_method = \"tsne\"\n",
    "\n",
    "if reduction_method == \"umap\":\n",
    "    image_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_image_features = image_reducer.fit_transform(image_features_array)\n",
    "    image_title = 'UMAP Visualization of CLIP Image Embeddings'\n",
    "elif reduction_method == \"tsne\":\n",
    "    image_reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, num_images - 1) if num_images > 1 else 1) # Adjust perplexity\n",
    "    reduced_image_features = image_reducer.fit_transform(image_features_array)\n",
    "    image_title = 't-SNE Visualization of CLIP Image Embeddings' \n",
    "else:\n",
    "    raise ValueError(\"Método de reducción no válido. Debe ser 'umap' o 'tsne'.\")\n",
    "\n",
    "\n",
    "# 4. Reducción de dimensionalidad con t-SNE para TEXTOS\n",
    "reduction_method = \"tsne\"\n",
    "\n",
    "if reduction_method == \"umap\":\n",
    "    text_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_text_features = text_reducer.fit_transform(text_features_array)\n",
    "    text_title = 'UMAP Visualization of CLIP Text Embeddings'\n",
    "elif reduction_method == \"tsne\":\n",
    "    text_reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, num_images - 1) if num_images > 1 else 1) # Adjust perplexity\n",
    "    reduced_text_features = text_reducer.fit_transform(text_features_array)\n",
    "    text_title = 't-SNE Visualization of CLIP Text Embeddings' \n",
    "else:\n",
    "    raise ValueError(\"Método de reducción no válido. Debe ser 'umap' o 'tsne'.\")\n",
    "\n",
    "\n",
    "# 5. Calcular la similaridad coseno entre pares de embeddings (imagenes y textos)\n",
    "similarity_matrix = cosine_similarity(image_features_array, text_features_array)\n",
    "\n",
    "# Número de imágenes a comparar\n",
    "num_comparisons = min(num_images, 8) # Aseguramos que no se compare mas de lo que hay\n",
    "\n",
    "# Iterar sobre las primeras num_comparisons imágenes\n",
    "print(\"\\nSimilitud Coseno:\")\n",
    "for i in range(num_comparisons):\n",
    "    similarity = similarity_matrix[i, i] # Compara la imagen i con el texto i\n",
    "    print(f\"Similaridad coseno entre la imagen {labels[i]} y su descripción: {similarity:.3f}\") # Formatted output\n",
    "\n",
    "\n",
    "print(\"\\nRango del coseno:\")\n",
    "print(\"El valor del coseno se mueve entre -1 y 1.\")\n",
    "print(\"-1 indica una similitud negativa perfecta (vectores opuestos).\")\n",
    "print(\"0 indica que no hay similitud (vectores ortogonales).\")\n",
    "print(\"1 indica una similitud positiva perfecta (vectores idénticos).\")\n",
    "\n",
    "# 6. Visualización en 2D para IMAGE FEATURES\n",
    "fig_image, ax_image = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Definir marcadores y colores para los grupos\n",
    "group_markers = {\"older\": \"o\", \"middle-aged\": \"s\", \"young\": \"^\", \"person\": \"D\"} \n",
    "group_colors = {\"older\": 'blue', \"middle-aged\": 'green', \"young\": 'red', \"person\": 'purple'} \n",
    "\n",
    "\n",
    "# Colorear imágenes por grupo\n",
    "for i in range(len(reduced_image_features)):\n",
    "    group = groups[i]\n",
    "    label = labels[i]\n",
    "    color = group_colors[group]\n",
    "    marker = group_markers[group]\n",
    "\n",
    "    ax_image.scatter(reduced_image_features[i, 0], reduced_image_features[i, 1], color=color, marker=marker, alpha=0.7, label=f'{group.capitalize()}' if i == 0 or group not in [groups[j] for j in range(i)] else None) # Label only once per group, label simplificado y capitalizado\n",
    "\n",
    "# Add text annotations for image points \n",
    "# for i in range(num_images):\n",
    "#     ax_image.text(reduced_image_features[i, 0], reduced_image_features[i, 1], labels[i], fontsize=8, alpha=0.7, color='black') # Color for text annotations - set to black\n",
    "\n",
    "# Create legend for groups (markers and colors)\n",
    "handles_group_image = []\n",
    "for group_name, color in group_colors.items():\n",
    "    marker = group_markers[group_name]\n",
    "    handles_group_image.append(mlines.Line2D([0], [0], marker=marker, color='w', label=f'{group_name.capitalize()}', markerfacecolor=color, markersize=10))\n",
    "\n",
    "# Add the group legend - place below label legend or adjust location\n",
    "ax_image.legend(handles=handles_group_image, title='Embedding Groups', loc='lower right')\n",
    "ax_image.set_title(image_title) # Usa image_title que ahora es de t-SNE\n",
    "ax_image.set_xlabel('t-SNE Dimension 1') # Eje X más específico\n",
    "ax_image.set_ylabel('t-SNE Dimension 2') # Eje Y más específico\n",
    "ax_image.grid(False) # Quitar la grid\n",
    "ax_image.set_xticks([]) # Quitar los valores del eje X\n",
    "ax_image.set_yticks([]) # Quitar los valores del eje Y\n",
    "plt.tight_layout()\n",
    "\n",
    "# 6. Visualización en 2D para TEXT FEATURES\n",
    "fig_text, ax_text = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Colorear textos por grupo\n",
    "for i in range(len(reduced_text_features)):\n",
    "    group = groups[i]\n",
    "    label = labels[i]\n",
    "    color = group_colors[group]\n",
    "    marker = group_markers[group]\n",
    "\n",
    "    ax_text.scatter(reduced_text_features[i, 0], reduced_text_features[i, 1], color=color, marker=marker, alpha=0.7, label=f'{group.capitalize()}' if i == 0 or group not in [groups[j] for j in range(i)] else None)\n",
    "\n",
    "# Create legend for groups (markers and colors)\n",
    "handles_group_text = []\n",
    "for group_name, color in group_colors.items():\n",
    "    marker = group_markers[group_name]\n",
    "    handles_group_text.append(mlines.Line2D([0], [0], marker=marker, color='w', label=f'{group_name.capitalize()}', markerfacecolor=color, markersize=10)) \n",
    "\n",
    "ax_text.legend(handles=handles_group_text, title='Embedding Groups', loc='lower right')\n",
    "ax_text.set_title(text_title) \n",
    "ax_text.set_xlabel('t-SNE Dimension 1') \n",
    "ax_text.set_ylabel('t-SNE Dimension 2')\n",
    "ax_text.grid(False) \n",
    "ax_text.set_xticks([]) \n",
    "ax_text.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9f2e9",
   "metadata": {},
   "source": [
    "# TSNE - BY GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_title = \"t-SNE Visualization of CLIP Image Embeddings\"\n",
    "text_title  = \"t-SNE Visualization of CLIP Text Embeddings\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Definir marcadores y colores para los grupos\n",
    "group_markers = {\"older\": \"o\", \"middle-aged\": \"s\", \"young\": \"^\", \"person\": \"D\"}\n",
    "group_colors  = {\"older\": 'blue', \"middle-aged\": 'green', \"young\": 'red', \"person\": 'purple'}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Función para dibujar el plot t-SNE resaltando un grupo\n",
    "def plot_tsne_highlighted(features, groups, labels, title, highlight_group, \n",
    "                          alpha_normal=0.7, alpha_other=0.1, legend_loc='lower right'):\n",
    "    \"\"\"\n",
    "    Dibuja un scatter plot de t-SNE donde el grupo 'highlight_group'\n",
    "    se muestra con alpha normal y los demás con mayor transparencia.\n",
    "    Se conserva el estilo (sin ejes, misma leyenda, etc.).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Dibujar cada punto: si pertenece al grupo destacado, usar alpha normal,\n",
    "    # en caso contrario, usar mayor transparencia.\n",
    "    for i in range(len(features)):\n",
    "        group = groups[i]\n",
    "        marker = group_markers[group]\n",
    "        color  = group_colors[group]\n",
    "        alpha  = alpha_normal if group == highlight_group else alpha_other\n",
    "        ax.scatter(features[i, 0], features[i, 1], color=color, marker=marker, alpha=alpha)\n",
    "    \n",
    "    # Crear la leyenda con todos los grupos\n",
    "    handles = []\n",
    "    for g in group_colors.keys():\n",
    "        marker = group_markers[g]\n",
    "        color  = group_colors[g]\n",
    "        handles.append(mlines.Line2D([0], [0],\n",
    "                                     marker=marker,\n",
    "                                     color='w',\n",
    "                                     label=g.capitalize(),\n",
    "                                     markerfacecolor=color,\n",
    "                                     markersize=10))\n",
    "    ax.legend(handles=handles, title='Embedding Groups', loc=legend_loc)\n",
    "    \n",
    "    # Configuración de ejes y título\n",
    "    ax.set_title(f\"{title} - Highlight: {highlight_group.capitalize()}\")\n",
    "    ax.set_xlabel(\"t-SNE Dimension 1\")\n",
    "    ax.set_ylabel(\"t-SNE Dimension 2\")\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Generar un plot para cada grupo en t-SNE (IMAGENES)\n",
    "for g in group_colors.keys():\n",
    "    plot_tsne_highlighted(reduced_image_features, groups, labels, image_title, highlight_group=g)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Generar un plot para cada grupo en t-SNE (TEXTOS)\n",
    "for g in group_colors.keys():\n",
    "    plot_tsne_highlighted(reduced_text_features, groups, labels, text_title, highlight_group=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b484536",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b59980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0624a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar el modelo CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# 2. Configuración de tus imágenes y etiquetas\n",
    "image_dir = \"generated_images/500p_quadruplets_v1/\"  \n",
    "num_images = 0\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        num_images += 1\n",
    "\n",
    "labels = []  # Lista para las etiquetas de imágenes\n",
    "descriptions = []  # Lista para las descripciones (textos)\n",
    "groups = [] # Lista para los grupos de imagenes\n",
    "\n",
    "image_features_list = [] # Lista para guardar solo los image features\n",
    "text_features_list = [] # Lista para guardar solo los text features\n",
    "\n",
    "# 3. Procesamiento de imágenes y etiquetas\n",
    "for i, filename in enumerate(os.listdir(image_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Carga la imagen\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except Image.UnidentifiedImageError: # Corrected exception type\n",
    "            print(f\"No se puede abrir o identificar la imagen: {filename}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocesa la imagen con CLIP\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Extrae el embedding de la imagen\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding de la imagen\n",
    "        image_features = image_features / np.linalg.norm(image_features)\n",
    "        image_features_list.append(image_features.flatten()) # Añade a la lista de image features\n",
    "\n",
    "        # Obtener el label del archivo. Esto puede variar segun como tengas los labels de tus archivos\n",
    "        label = filename.split(\".\")[0] # Usamos el nombre del archivo sin extension como label\n",
    "        labels.append(label)\n",
    "\n",
    "        # Crea la descripción basada en tu label (Ajusta esto según tus necesidades)\n",
    "        description = f\"{label}\"\n",
    "        descriptions.append(description)\n",
    "\n",
    "        # Prepara la entrada de texto con CLIP\n",
    "        text_input = clip.tokenize([description]).to(device)\n",
    "\n",
    "        # Extrae el embedding del texto\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding del texto\n",
    "        text_features = text_features / np.linalg.norm(text_features)\n",
    "        text_features_list.append(text_features.flatten()) # Añade a la lista de text features\n",
    "\n",
    "        # Clasifica la imagen en un grupo (Ajusta esto según tu estructura de nombres de archivo)\n",
    "        if \"older\" in label.lower():\n",
    "            group = \"older\"\n",
    "        elif \"middle\" in label.lower():\n",
    "            group = \"middle-aged\"\n",
    "        elif \"young\" in label.lower():\n",
    "            group = \"young\"\n",
    "        elif \"person\" in label.lower():\n",
    "            group = \"person\"\n",
    "        else:\n",
    "            group = \"unknown\" # Default group if none of the keywords are found\n",
    "        groups.append(group)\n",
    "\n",
    "print(\"Verificación de la clasificación de grupos:\")\n",
    "for i in range(len(labels)):\n",
    "    print(f\"Label: {labels[i]}, Group: {groups[i]}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "image_features_array = np.array(image_features_list)\n",
    "text_features_array = np.array(text_features_list)\n",
    "\n",
    "# 4. Reducción de dimensionalidad con UMAP o t-SNE para IMAGENES\n",
    "reduction_method = \"umap\" \n",
    "\n",
    "if reduction_method == \"umap\":\n",
    "    image_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_image_features = image_reducer.fit_transform(image_features_array)\n",
    "    image_title = 'UMAP Visualization of CLIP Image Embeddings'\n",
    "elif reduction_method == \"tsne\":\n",
    "    image_reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, num_images - 1) if num_images > 1 else 1) # Adjust perplexity\n",
    "    reduced_image_features = image_reducer.fit_transform(image_features_array)\n",
    "    image_title = 't-SNE Visualization of CLIP Image Embeddings'\n",
    "else:\n",
    "    raise ValueError(\"Método de reducción no válido. Debe ser 'umap' o 'tsne'.\")\n",
    "\n",
    "# 4. Reducción de dimensionalidad con UMAP o t-SNE para TEXTOS\n",
    "reduction_method = \"umap\" \n",
    "\n",
    "if reduction_method == \"umap\":\n",
    "    text_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_text_features = text_reducer.fit_transform(text_features_array)\n",
    "    text_title = 'UMAP Visualization of CLIP Text Embeddings'\n",
    "elif reduction_method == \"tsne\":\n",
    "    text_reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, num_images - 1) if num_images > 1 else 1) \n",
    "    reduced_text_features = text_reducer.fit_transform(text_features_array)\n",
    "    text_title = 't-SNE Visualization of CLIP Text Embeddings'\n",
    "else:\n",
    "    raise ValueError(\"Método de reducción no válido. Debe ser 'umap' o 'tsne'.\")\n",
    "\n",
    "# 5. Calcular la similaridad coseno entre pares de embeddings (imagenes y textos)\n",
    "similarity_matrix = cosine_similarity(image_features_array, text_features_array)\n",
    "\n",
    "# Número de imágenes a comparar\n",
    "num_comparisons = min(num_images, 8) \n",
    "\n",
    "# Iterar sobre las primeras num_comparisons imágenes\n",
    "print(\"\\nSimilitud Coseno:\")\n",
    "for i in range(num_comparisons):\n",
    "    similarity = similarity_matrix[i, i] # Compara la imagen i con el texto i\n",
    "    print(f\"Similaridad coseno entre la imagen {labels[i]} y su descripción: {similarity:.3f}\") # Formatted output\n",
    "\n",
    "print(\"\\nRango del coseno:\")\n",
    "print(\"El valor del coseno se mueve entre -1 y 1.\")\n",
    "print(\"-1 indica una similitud negativa perfecta (vectores opuestos).\")\n",
    "print(\"0 indica que no hay similitud (vectores ortogonales).\")\n",
    "print(\"1 indica una similitud positiva perfecta (vectores idénticos).\")\n",
    "\n",
    "# 6. Visualización en 2D para IMAGE FEATURES\n",
    "fig_image, ax_image = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Definir marcadores y colores para los grupos\n",
    "group_markers = {\"older\": \"o\", \"middle-aged\": \"s\", \"young\": \"^\", \"person\": \"D\"} # More markers for groups\n",
    "group_colors = {\"older\": 'blue', \"middle-aged\": 'green', \"young\": 'red', \"person\": 'purple'} # Colors for groups\n",
    "\n",
    "# Colorear imágenes por grupo\n",
    "for i in range(len(reduced_image_features)):\n",
    "    group = groups[i]\n",
    "    label = labels[i]\n",
    "    color = group_colors[group]\n",
    "    marker = group_markers[group]\n",
    "\n",
    "    ax_image.scatter(reduced_image_features[i, 0], reduced_image_features[i, 1], color=color, marker=marker, alpha=0.7, label=f'Image Embeddings ({group})' if i == 0 or group not in [groups[j] for j in range(i)] else None) # Label only once per group\n",
    "\n",
    "# Add text annotations for image points\n",
    "# for i in range(num_images):\n",
    "#     ax_image.text(reduced_image_features[i, 0], reduced_image_features[i, 1], labels[i], fontsize=8, alpha=0.7, color='black') # Color for text annotations - set to black\n",
    "\n",
    "# Create legend for groups (markers and colors)\n",
    "handles_group_image = []\n",
    "for group_name, color in group_colors.items():\n",
    "    marker = group_markers[group_name]\n",
    "    handles_group_image.append(plt.Line2D([0], [0], marker=marker, color='w', label=f'Image ({group_name})', markerfacecolor=color, markersize=10))\n",
    "\n",
    "# Add the group legend - place below label legend or adjust location\n",
    "ax_image.legend(handles=handles_group_image, title='Image Embedding Groups', loc='lower left') # Adjust location as needed\n",
    "\n",
    "ax_image.set_title(image_title)\n",
    "ax_image.set_xlabel('Dimension 1')\n",
    "ax_image.set_ylabel('Dimension 2')\n",
    "ax_image.grid(False)\n",
    "ax_image.set_xticks([])\n",
    "ax_image.set_yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "# 6. Visualización en 2D para TEXT FEATURES\n",
    "fig_text, ax_text = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Definir marcadores y colores para los grupos (reutilizamos los de arriba)\n",
    "# group_markers = {\"older\": \"o\", \"middle-aged\": \"s\", \"young\": \"^\", \"person\": \"x\", \"unknown\": \"d\"} # More markers for groups\n",
    "# group_colors = {\"older\": 'blue', \"middle-aged\": 'green', \"young\": 'red', \"person\": 'purple', \"unknown\": 'gray'} # Colors for groups\n",
    "\n",
    "# Colorear textos por grupo\n",
    "for i in range(len(reduced_text_features)):\n",
    "    group = groups[i]\n",
    "    label = labels[i]\n",
    "    color = group_colors[group]\n",
    "    marker = group_markers[group]\n",
    "\n",
    "    ax_text.scatter(reduced_text_features[i, 0], reduced_text_features[i, 1], color=color, marker=marker, alpha=0.7, label=f'Text Embeddings ({group})' if i == 0 or group not in [groups[j] for j in range(i)] else None) # Label only once per group\n",
    "\n",
    "# Add text annotations for text points\n",
    "# for i in range(num_images):\n",
    "#     ax_text.text(reduced_text_features[i, 0], reduced_text_features[i, 1], labels[i], fontsize=8, alpha=0.7, color='black') # Color for text annotations - set to black\n",
    "\n",
    "# Create legend for groups (markers and colors)\n",
    "handles_group_text = []\n",
    "for group_name, color in group_colors.items():\n",
    "    marker = group_markers[group_name]\n",
    "    handles_group_text.append(plt.Line2D([0], [0], marker=marker, color='w', label=f'Text ({group_name})', markerfacecolor=color, markersize=10))\n",
    "\n",
    "# Add the group legend - place below label legend or adjust location\n",
    "ax_text.legend(handles=handles_group_text, title='Text Embedding Groups', loc='lower right') # Adjust location as needed\n",
    "ax_text.set_title(text_title)\n",
    "ax_text.set_xlabel('Dimension 1')\n",
    "ax_text.set_ylabel('Dimension 2')\n",
    "ax_text.set_xticks([])\n",
    "ax_text.set_yticks([])\n",
    "ax_text.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fdf4c",
   "metadata": {},
   "source": [
    "# UMAP - BY GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d00855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "image_title = \"UMAP Visualization of CLIP Image Embeddings\"\n",
    "text_title  = \"UMAP Visualization of CLIP Text Embeddings\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# group_markers = {\"older\": \"o\", \"middle-aged\": \"s\", \"young\": \"^\", \"person\": \"D\"}\n",
    "# group_colors  = {\"older\": 'blue', \"middle-aged\": 'green', \"young\": 'red', \"person\": 'purple'}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Función para dibujar un plot destacando un grupo en 'reduced_features'\n",
    "def plot_features_highlighted(\n",
    "    reduced_features,\n",
    "    groups,\n",
    "    labels,\n",
    "    group_markers,\n",
    "    group_colors,\n",
    "    title,\n",
    "    highlight_group,\n",
    "    alpha_other=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Dibuja un scatter plot donde 'highlight_group' se muestra con alpha=0.7\n",
    "    y el resto de grupos con alpha=alpha_other. Mantiene misma leyenda.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Pintar puntos: si pertenecen al grupo destacado -> alpha normal (0.7),\n",
    "    # si no -> alpha reducido (por defecto 0.1)\n",
    "    for i in range(len(reduced_features)):\n",
    "        group = groups[i]\n",
    "        marker = group_markers[group]\n",
    "        color  = group_colors[group]\n",
    "        alpha  = 0.7 if group == highlight_group else alpha_other\n",
    "        \n",
    "        ax.scatter(\n",
    "            reduced_features[i, 0],\n",
    "            reduced_features[i, 1],\n",
    "            color=color,\n",
    "            marker=marker,\n",
    "            alpha=alpha\n",
    "        )\n",
    "    \n",
    "    handles = []\n",
    "    for g_name, g_color in group_colors.items():\n",
    "        g_marker = group_markers[g_name]\n",
    "        handles.append(\n",
    "            mlines.Line2D(\n",
    "                [0], [0],\n",
    "                marker=g_marker,\n",
    "                color='w',\n",
    "                label=f'{g_name}',\n",
    "                markerfacecolor=g_color,\n",
    "                markersize=10\n",
    "            )\n",
    "        )\n",
    "    ax.legend(handles=handles, title='Groups', loc='lower left') \n",
    "    \n",
    "    ax.set_title(f\"{title} - Highlight: {highlight_group}\")\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Generar un plot por cada grupo para IMAGE FEATURES\n",
    "for g in group_colors.keys():\n",
    "    plot_features_highlighted(\n",
    "        reduced_features=reduced_image_features,\n",
    "        groups=groups,\n",
    "        labels=labels,\n",
    "        group_markers=group_markers,\n",
    "        group_colors=group_colors,\n",
    "        title=image_title,\n",
    "        highlight_group=g,\n",
    "        alpha_other=0.1\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Generar un plot por cada grupo para TEXT FEATURES\n",
    "for g in group_colors.keys():\n",
    "    plot_features_highlighted(\n",
    "        reduced_features=reduced_text_features,\n",
    "        groups=groups,\n",
    "        labels=labels,\n",
    "        group_markers=group_markers,\n",
    "        group_colors=group_colors,\n",
    "        title=text_title,\n",
    "        highlight_group=g,\n",
    "        alpha_other=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74321bdc",
   "metadata": {},
   "source": [
    "# INTERACTIVE UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ed37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import umap\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 1. Cargar el modelo CLIP\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ecc68",
   "metadata": {},
   "source": [
    "## IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"generated_images/500p_quadruplets_v1/\"  \n",
    "num_images = 0\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        num_images += 1\n",
    "\n",
    "labels = []  # Lista para las etiquetas de imágenes\n",
    "groups = []  # Lista para los grupos de edad\n",
    "activities = []  # Lista para las actividades\n",
    "image_features_list = []  # Lista para guardar los image features\n",
    "\n",
    "def extract_activity(filename):\n",
    "    filename_lower = filename.lower()\n",
    "    \n",
    "    # Patrón para middle-aged\n",
    "    middle_aged_pattern = r'middle-aged_person_([a-z_]+)\\.'\n",
    "    match = re.search(middle_aged_pattern, filename_lower)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).replace('_', ' ')\n",
    "    \n",
    "    # Patrones para otros grupos\n",
    "    group_patterns = {\n",
    "        'young_person_': r'young_person_([a-z_]+)\\.',\n",
    "        'older_person_': r'older_person_([a-z_]+)\\.',\n",
    "        'person_': r'person_([a-z_]+)\\.'  # Para el grupo \"person\" sin edad\n",
    "    }\n",
    "    \n",
    "    for pattern in group_patterns.values():\n",
    "        match = re.search(pattern, filename_lower)\n",
    "        if match:\n",
    "            return match.group(1).replace('_', ' ')\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "print(\"Procesando imágenes y extrayendo embeddings...\")\n",
    "for i, filename in enumerate(os.listdir(image_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Carga la imagen\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        except Image.UnidentifiedImageError:\n",
    "            print(f\"No se puede abrir o identificar la imagen: {filename}. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocesa la imagen con CLIP\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Extrae el embedding de la imagen\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding de la imagen\n",
    "        image_features = image_features / np.linalg.norm(image_features)\n",
    "        image_features_list.append(image_features.flatten())\n",
    "\n",
    "        # Obtener el label del archivo\n",
    "        label = filename.split(\".\")[0]  # Usamos el nombre del archivo sin extension como label\n",
    "        labels.append(label)\n",
    "\n",
    "        # Obtener la actividad\n",
    "        activity = extract_activity(filename)\n",
    "        activities.append(activity)\n",
    "\n",
    "        label_lower = filename.lower()  # Usamos el nombre del archivo en minúsculas\n",
    "\n",
    "        if \"middle-aged\" in label_lower or \"middle_aged\" in label_lower:\n",
    "            group = \"middle-aged\"\n",
    "        elif \"older\" in label_lower:\n",
    "            group = \"older\"\n",
    "        elif \"middle\" in label_lower:\n",
    "            group = \"middle-aged\"\n",
    "        elif \"young\" in label_lower:\n",
    "            group = \"young\"\n",
    "        elif \"person\" in label_lower:\n",
    "            group = \"person\"\n",
    "        else:\n",
    "            group = \"unknown\"\n",
    "        groups.append(group)\n",
    "\n",
    "unique_activities = sorted(list(set(activities)))\n",
    "print(f\"Total de imágenes procesadas: {len(labels)}\")\n",
    "print(f\"Actividades detectadas: {len(unique_activities)}\")\n",
    "print(f\"Ejemplos de actividades: {', '.join(unique_activities[:5])}\")\n",
    "\n",
    "print(\"\\nEstadísticas por grupo de edad:\")\n",
    "for group_name in group_colors.keys():\n",
    "    if group_name in groups:\n",
    "        group_indices = [i for i, g in enumerate(groups) if g == group_name]\n",
    "        group_activities = [activities[i] for i in group_indices]\n",
    "        print(f\"Grupo '{group_name}': {len(group_indices)} imágenes\")\n",
    "        print(f\"  Ejemplos de actividades: {', '.join(sorted(set(group_activities))[:5])}\")\n",
    "        print(f\"  Ejemplos de archivos: {', '.join([labels[i] for i in group_indices[:3]])}\")\n",
    "\n",
    "print(\"Aplicando reducción dimensional con UMAP...\")\n",
    "image_features_array = np.array(image_features_list)\n",
    "image_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "reduced_image_features = image_reducer.fit_transform(image_features_array)\n",
    "\n",
    "group_colors = {\n",
    "    \"older\": 'blue',\n",
    "    \"middle-aged\": 'green',\n",
    "    \"young\": 'red',\n",
    "    \"person\": 'purple',\n",
    "    \"unknown\": 'gray'\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "base_traces = []\n",
    "for group in group_colors.keys():\n",
    "    if group not in groups:\n",
    "        continue\n",
    "\n",
    "    indices = [i for i, g in enumerate(groups) if g == group]\n",
    "\n",
    "    trace = go.Scatter(\n",
    "        x=reduced_image_features[indices, 0],\n",
    "        y=reduced_image_features[indices, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=group_colors[group],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f\"Label: {labels[i]}<br>Group: {groups[i]}<br>Activity: {activities[i]}\" for i in indices],\n",
    "        name=f\"{group}\",\n",
    "        customdata=[i for i in indices],  \n",
    "        hoverinfo='text',\n",
    "        visible=True\n",
    "    )\n",
    "    fig.add_trace(trace)\n",
    "    base_traces.append(trace)\n",
    "\n",
    "highlight_traces = []\n",
    "for activity in unique_activities:\n",
    "    activity_indices = [idx for idx, act in enumerate(activities) if act == activity]\n",
    "\n",
    "    if activity_indices:\n",
    "        trace = go.Scatter(\n",
    "            x=reduced_image_features[activity_indices, 0],\n",
    "            y=reduced_image_features[activity_indices, 1],\n",
    "            mode='markers+text',\n",
    "            marker=dict(\n",
    "                size=15,\n",
    "                color='yellow',\n",
    "                opacity=1,\n",
    "                line=dict(width=2, color='black')\n",
    "            ),\n",
    "            text=[labels[idx] for idx in activity_indices],\n",
    "            textposition=\"top center\",\n",
    "            name=f\"Highlighted - {activity}\",\n",
    "            visible=False  \n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "        highlight_traces.append(trace)\n",
    "\n",
    "buttons = []\n",
    "button_all = dict(\n",
    "    args=[{\n",
    "        'visible': [True] * len(base_traces) + [False] * len(highlight_traces)\n",
    "    }],\n",
    "    label=\"All Activities\",\n",
    "    method=\"update\"\n",
    ")\n",
    "buttons.append(button_all)\n",
    "\n",
    "for i, activity in enumerate(unique_activities):\n",
    "    visibility = [True] * len(base_traces) \n",
    "    highlight_visibility = [False] * len(highlight_traces)  \n",
    "\n",
    "    highlight_visibility[i] = True\n",
    "\n",
    "    button = dict(\n",
    "        args=[{\n",
    "            'visible': visibility + highlight_visibility\n",
    "        }],\n",
    "        label=activity.capitalize(),\n",
    "        method=\"update\"\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"dropdown\",\n",
    "            buttons=buttons,\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=1.3, \n",
    "            xanchor=\"right\", \n",
    "            y=1.01,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Interactive visualization of CLIP (ViT-B/32) Image Embeddings by Activity</b>\",\n",
    "        x=0.4,\n",
    "        xanchor=\"center\"\n",
    "    ),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"<b>Select an activity:</b>\",\n",
    "            x=1.2,\n",
    "            y=1.03, \n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            showarrow=False,\n",
    "            xanchor=\"right\" \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,  \n",
    "    width=1400,   \n",
    "    margin=dict(t=100, r=200,b=100),  \n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.08,  \n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(showticklabels=False, showgrid=True, zeroline=True)\n",
    "fig.update_yaxes(showticklabels=False, showgrid=True, zeroline=False)\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"500p_image_embeddings_umap_activity_interactive_042225.html\")\n",
    "\n",
    "print(\"\\nSe ha generado la visualización interactiva de Image Embeddings.\")\n",
    "print(\"Usa el menú desplegable para seleccionar una actividad específica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7a9cb",
   "metadata": {},
   "source": [
    "## TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a52c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"generated_images/500p_quadruplets_v1\" \n",
    "# num_images = 0\n",
    "# for filename in os.listdir(image_dir):\n",
    "#     if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "#         num_images += 1\n",
    "\n",
    "labels = []  # Lista para las etiquetas de imágenes\n",
    "descriptions = []  # Lista para las descripciones (textos)\n",
    "groups = []  # Lista para los grupos de edad\n",
    "activities = []  # Lista para las actividades\n",
    "text_features_list = []  # Lista para guardar los text features\n",
    "\n",
    "# def extract_activity(filename):\n",
    "#     filename_lower = filename.lower()\n",
    "    \n",
    "#     middle_aged_pattern = r'middle-aged_person_([a-z_]+)\\.'\n",
    "#     match = re.search(middle_aged_pattern, filename_lower)\n",
    "    \n",
    "#     if match:\n",
    "#         return match.group(1).replace('_', ' ')\n",
    "    \n",
    "#     group_patterns = {\n",
    "#         'young_person_': r'young_person_([a-z_]+)\\.',\n",
    "#         'older_person_': r'older_person_([a-z_]+)\\.',\n",
    "#         'person_': r'person_([a-z_]+)\\.'  # Para el grupo \"person\" sin edad\n",
    "#     }\n",
    "    \n",
    "#     for pattern in group_patterns.values():\n",
    "#         match = re.search(pattern, filename_lower)\n",
    "#         if match:\n",
    "#             return match.group(1).replace('_', ' ')\n",
    "    \n",
    "#     return \"unknown\"\n",
    "\n",
    "# 3. Procesamiento de textos\n",
    "print(\"Procesando textos y extrayendo embeddings...\")\n",
    "for i, filename in enumerate(os.listdir(image_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        # Obtener el label del archivo\n",
    "        label = filename.split(\".\")[0]  # Usamos el nombre del archivo sin extension como label\n",
    "        labels.append(label)\n",
    "\n",
    "        # Obtener la actividad\n",
    "        activity = extract_activity(filename)\n",
    "        activities.append(activity)\n",
    "\n",
    "        # Crea la descripción basada en tu label\n",
    "        description = f\"{label}\"\n",
    "        descriptions.append(description)\n",
    "\n",
    "        # Prepara la entrada de texto con CLIP\n",
    "        text_input = clip.tokenize([description]).to(device)\n",
    "\n",
    "        # Extrae el embedding del texto\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_input).cpu().numpy()\n",
    "\n",
    "        # Normaliza el embedding del texto\n",
    "        text_features = text_features / np.linalg.norm(text_features)\n",
    "        text_features_list.append(text_features.flatten())\n",
    "\n",
    "        # Clasificar en un grupo de edad\n",
    "        label_lower = filename.lower()  # Usamos el nombre del archivo en minúsculas\n",
    "\n",
    "        if \"middle-aged\" in label_lower or \"middle_aged\" in label_lower:\n",
    "            group = \"middle-aged\"\n",
    "        elif \"older\" in label_lower:\n",
    "            group = \"older\"\n",
    "        elif \"middle\" in label_lower:\n",
    "            group = \"middle-aged\"\n",
    "        elif \"young\" in label_lower:\n",
    "            group = \"young\"\n",
    "        elif \"person\" in label_lower:\n",
    "            group = \"person\"\n",
    "        else:\n",
    "            group = \"unknown\"\n",
    "        groups.append(group)\n",
    "\n",
    "unique_activities = sorted(list(set(activities)))\n",
    "print(f\"Total de textos procesados: {len(labels)}\")\n",
    "print(f\"Actividades detectadas: {len(unique_activities)}\")\n",
    "print(f\"Ejemplos de actividades: {', '.join(unique_activities[:5])}\")\n",
    "\n",
    "print(\"\\nEstadísticas por grupo de edad:\")\n",
    "for group_name in group_colors.keys():\n",
    "    if group_name in groups:\n",
    "        group_indices = [i for i, g in enumerate(groups) if g == group_name]\n",
    "        group_activities = [activities[i] for i in group_indices]\n",
    "        print(f\"Grupo '{group_name}': {len(group_indices)} textos\")\n",
    "        print(f\"  Ejemplos de actividades: {', '.join(sorted(set(group_activities))[:5])}\")\n",
    "        print(f\"  Ejemplos de archivos: {', '.join([labels[i] for i in group_indices[:3]])}\")\n",
    "\n",
    "print(\"Aplicando reducción dimensional con UMAP...\")\n",
    "text_features_array = np.array(text_features_list)\n",
    "text_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "reduced_text_features = text_reducer.fit_transform(text_features_array)\n",
    "\n",
    "group_colors = {\n",
    "    \"older\": 'blue',\n",
    "    \"middle-aged\": 'green',\n",
    "    \"young\": 'red',\n",
    "    \"person\": 'purple',\n",
    "    \"unknown\": 'gray'\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "base_traces = []\n",
    "for group in group_colors.keys():\n",
    "    if group not in groups:\n",
    "        continue\n",
    "\n",
    "    indices = [i for i, g in enumerate(groups) if g == group]\n",
    "\n",
    "    trace = go.Scatter(\n",
    "        x=reduced_text_features[indices, 0],\n",
    "        y=reduced_text_features[indices, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=10,\n",
    "            color=group_colors[group],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f\"Label: {labels[i]}<br>Group: {groups[i]}<br>Activity: {activities[i]}\" for i in indices],\n",
    "        name=f\"{group}\",\n",
    "        customdata=[i for i in indices],  \n",
    "        hoverinfo='text',\n",
    "        visible=True\n",
    "    )\n",
    "    fig.add_trace(trace)\n",
    "    base_traces.append(trace)\n",
    "\n",
    "highlight_traces = []\n",
    "for activity in unique_activities:\n",
    "    activity_indices = [idx for idx, act in enumerate(activities) if act == activity]\n",
    "\n",
    "    # Si tenemos prompts para esta actividad\n",
    "    if activity_indices:\n",
    "        trace = go.Scatter(\n",
    "            x=reduced_text_features[activity_indices, 0],\n",
    "            y=reduced_text_features[activity_indices, 1],\n",
    "            mode='markers+text',\n",
    "            marker=dict(\n",
    "                size=15,\n",
    "                color='yellow',\n",
    "                opacity=1,\n",
    "                line=dict(width=2, color='black')\n",
    "            ),\n",
    "            text=[labels[idx] for idx in activity_indices],\n",
    "            textposition=\"top center\",\n",
    "            name=f\"Highlighted - {activity}\",\n",
    "            visible=False \n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "        highlight_traces.append(trace)\n",
    "\n",
    "buttons = []\n",
    "\n",
    "button_all = dict(\n",
    "    args=[{\n",
    "        'visible': [True] * len(base_traces) + [False] * len(highlight_traces)\n",
    "    }],\n",
    "    label=\"All Activities\",\n",
    "    method=\"update\"\n",
    ")\n",
    "buttons.append(button_all)\n",
    "\n",
    "for i, activity in enumerate(unique_activities):\n",
    "    visibility = [True] * len(base_traces)  # Trazas base siempre visibles\n",
    "    highlight_visibility = [False] * len(highlight_traces)  # Todas las trazas destacadas ocultas por defecto\n",
    "\n",
    "    highlight_visibility[i] = True\n",
    "\n",
    "    button = dict(\n",
    "        args=[{\n",
    "            'visible': visibility + highlight_visibility\n",
    "        }],\n",
    "        label=activity.capitalize(),\n",
    "        method=\"update\"\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"dropdown\",\n",
    "            buttons=buttons,\n",
    "            direction=\"down\",\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=1.3, \n",
    "            xanchor=\"right\", \n",
    "            y=1.01,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Interactive visualization of CLIP (ViT-B/32) Text Embeddings by Activity</b>\",\n",
    "        x=0.4,\n",
    "        xanchor=\"center\"\n",
    "    ),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"<b>Select an activity:</b>\",\n",
    "            x=1.2, \n",
    "            y=1.03, \n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            showarrow=False,\n",
    "            xanchor=\"right\" \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,  \n",
    "    width=1400,   \n",
    "    margin=dict(t=100, r=200,b=100),  \n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.08,  \n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_xaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"500p_text_embeddings_umap_activity_interactive_042225.html\")\n",
    "\n",
    "print(\"\\nSe ha generado la visualización interactiva de Text Embeddings.\")\n",
    "print(\"Usa el menú desplegable para seleccionar una actividad específica.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
